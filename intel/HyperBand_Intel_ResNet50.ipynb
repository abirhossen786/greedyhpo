{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.tuners import Hyperband\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_random_image(target_dir, target_class):\n",
    "    target_folder = target_dir + target_class\n",
    "    random_image = random.sample(os.listdir(target_folder), 1)\n",
    "    img = mpimg.imread(target_folder+'/'+random_image[0])\n",
    "    plt.imshow(img)\n",
    "    plt.title(target_class)\n",
    "    plt.axis('off');\n",
    "    #print(f\"Image shape {img.shape}\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "class_names=['buildings', 'forest', 'glacier', 'mountain' ,'sea' ,'street']\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(18):\n",
    "    plt.subplot(3, 6, i+1)\n",
    "    class_name = random.choice(class_names)\n",
    "    img = view_random_image(target_dir='/home/abir/Downloads/greedyhpo/intel/archive/seg_train/seg_train/',target_class=class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/abir/Downloads/greedyhpo/intel/archive/seg_train/seg_train\"\n",
    "test_dir = \"/home/abir/Downloads/greedyhpo/intel/archive/seg_test/seg_test\"\n",
    "val_dir =\"/home/abir/Downloads/greedyhpo/intel/archive/seg_val/seg_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_data = train_data.flow_from_directory(train_dir,\n",
    "                                              batch_size = 32,\n",
    "                                              target_size = (32,32), \n",
    "                                              class_mode = 'categorical')\n",
    "\n",
    "val_data = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "validation_data = val_data.flow_from_directory(val_dir,\n",
    "                                            batch_size = 32,\n",
    "                                            target_size = (32,32), \n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "\n",
    "test_data = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "testing_data = test_data.flow_from_directory(test_dir,\n",
    "                                            batch_size = 32,\n",
    "                                            target_size = (32,32), \n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = f\"{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "   \n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])# SKIP Connection\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_shape=(32, 32, 3)\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding2D((1, 1))(X_input)\n",
    "\n",
    "    X = Conv2D(hp.Choice('s1_filter_size_1', values=[16,32,64,128,256,512],default=16), (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[hp.Choice('s2_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s2_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s2_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [hp.Choice('s2_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s2_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s2_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [hp.Choice('s2_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s2_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s2_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=2, block='c')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[hp.Choice('s3_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [hp.Choice('s3_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [hp.Choice('s3_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [hp.Choice('s3_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s3_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=3, block='d')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[hp.Choice('s4_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [hp.Choice('s4_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [hp.Choice('s4_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [hp.Choice('s4_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [hp.Choice('s4_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [hp.Choice('s4_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s4_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=4, block='f')\n",
    "\n",
    "    X = X = convolutional_block(X, f=3, filters=[hp.Choice('s5_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s5_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s5_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [hp.Choice('s5_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s5_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s5_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [hp.Choice('s5_filter_size_1', values=[16,32,64,128,256,512],default=16), hp.Choice('s5_filter_size_2', values=[16,32,64,128,256,512],default=16), hp.Choice('s5_filter_size_3', values=[16,32,64,128,256,512],default=128)], stage=5, block='c')\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
    "\n",
    "    X= Dropout(hp.Choice('dp_size_1', values=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],default=0.0))(X)\n",
    "    X = Flatten()(X)\n",
    "    X=Dense(hp.Choice('dense_size_1', values=[64,128,256,512],default=64), activation='relu',kernel_regularizer=regularizers.l2(hp.Choice('wd_size_1', values=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],default=0.0)), name='fc1',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X=Dense(hp.Choice('dense_size_2', values=[64,128,256,512],default=64), activation='relu',kernel_regularizer=regularizers.l2(hp.Choice('wd_size_2', values=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],default=0.0)), name='fc2',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X= Dropout(hp.Choice('dp_size_2', values=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],default=0.0))(X)\n",
    "    X = Dense(6,activation='softmax', name='fc3',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    my_callbacks = [ tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "    opt = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-1,1e-2, 1e-3, 1e-4, 1e-5],default=1e-2))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                                    optimizer=opt,\n",
    "                                    metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    tuner = Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=100,  # how many model variations to test?\n",
    "        executions_per_trial=1,  # how many trials per variation? (same model could perform differently)\n",
    "        directory=LOG_DIR,\n",
    "        project_name='HB_ResNet50_Intel')\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    my_callbacks = [ tf.keras.callbacks.EarlyStopping(patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(training_data,\n",
    "             verbose=1, # just slapping this here bc jupyter notebook. The console out was getting messy.\n",
    "             epochs=100,\n",
    "             batch_size=64,\n",
    "             callbacks=[my_callbacks],  # if you have callbacks like tensorboard, they go here.\n",
    "             validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()\n",
    "with open(f\"tuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tuner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    hist=model.fit(training_data,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_data=validation_data,\n",
    "          callbacks=[my_callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,test_acc = model.evaluate(testing_data[0][0],testing_data[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
