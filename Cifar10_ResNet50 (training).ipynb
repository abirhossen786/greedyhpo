{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2,keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import glorot_uniform\n",
    "import visualkeras\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import seaborn as sns\n",
    "\n",
    "# from lib import proc_img\n",
    "# from lib import create_gen\n",
    "\n",
    "def printmd(string):\n",
    "    #Print with Markdowns    \n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_images, train_labels, test_size=0.2, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "   \n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])# SKIP Connection\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Greedy\n",
    "# learning rate=  0.001 \n",
    "# dense_size_1=  64 \n",
    "# dense_size_2=  64 \n",
    "# s1_filter_size_1=  16 \n",
    "# s2_filter_size_1=  16 \n",
    "# s2_filter_size_2=  16 \n",
    "# s2_filter_size_3=  128 \n",
    "# s3_filter_size_1=  16 \n",
    "# s3_filter_size_2=  16 \n",
    "# s3_filter_size_3=  128 \n",
    "# s4_filter_size_1=  16 \n",
    "# s4_filter_size_2=  16 \n",
    "# s4_filter_size_3=  128 \n",
    "# s5_filter_size_1=  16 \n",
    "# s5_filter_size_2=  16 \n",
    "# s5_filter_size_3=  128 \n",
    "# dp_size_1=  0.0 \n",
    "# dp_size_2=  0.0 \n",
    "# wd_size_1=  0.0 \n",
    "# wd_size_2=  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Random Search\n",
    "# s1_filter_size_1= 64\n",
    "# s2_filter_size_1= 256\n",
    "# s2_filter_size_2= 128\n",
    "# s2_filter_size_3= 64\n",
    "# s3_filter_size_1= 512\n",
    "# s3_filter_size_2= 64\n",
    "# s3_filter_size_3= 128\n",
    "# s4_filter_size_1= 32\n",
    "# s4_filter_size_2= 16\n",
    "# s4_filter_size_3= 128\n",
    "# s5_filter_size_1= 32\n",
    "# s5_filter_size_2= 32\n",
    "# s5_filter_size_3= 512\n",
    "# dp_size_1= 0.4\n",
    "# dense_size_1= 64\n",
    "# wd_size_1= 0.0\n",
    "# dense_size_2= 256\n",
    "# wd_size_2= 0.0\n",
    "# dp_size_2= 0.0\n",
    "# learning_rate= 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bayesian\n",
    "# s1_filter_size_1= 64\n",
    "# s2_filter_size_1= 512\n",
    "# s2_filter_size_2= 256\n",
    "# s2_filter_size_3= 512\n",
    "# s3_filter_size_1= 512\n",
    "# s3_filter_size_2= 16\n",
    "# s3_filter_size_3= 512\n",
    "# s4_filter_size_1= 64\n",
    "# s4_filter_size_2= 128\n",
    "# s4_filter_size_3= 16\n",
    "# s5_filter_size_1= 16\n",
    "# s5_filter_size_2= 16\n",
    "# s5_filter_size_3= 16\n",
    "# dp_size_1= 0.0\n",
    "# dense_size_1= 64\n",
    "# wd_size_1= 0.8\n",
    "# dense_size_2= 128\n",
    "# wd_size_2= 0.0\n",
    "# dp_size_2= 0.1\n",
    "# learning_rate= 0.001\n",
    "\n",
    "                                                                             \n",
    "input_shape=(32, 32, 3)\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "X = ZeroPadding2D((1, 1))(X_input)\n",
    "\n",
    "X = Conv2D(s1_filter_size_1, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "X = convolutional_block(X, f=3, filters=[s2_filter_size_1, s2_filter_size_2, s2_filter_size_3], stage=2, block='a', s=1)\n",
    "X = identity_block(X, 3, [s2_filter_size_1, s2_filter_size_2, s2_filter_size_3], stage=2, block='b')\n",
    "X = identity_block(X, 3, [s2_filter_size_1, s2_filter_size_2, s2_filter_size_3], stage=2, block='c')\n",
    "\n",
    "X = convolutional_block(X, f=3, filters=[s3_filter_size_1, s3_filter_size_2, s3_filter_size_3], stage=3, block='a', s=2)\n",
    "X = identity_block(X, 3, [s3_filter_size_1, s3_filter_size_2, s3_filter_size_3], stage=3, block='b')\n",
    "X = identity_block(X, 3, [s3_filter_size_1, s3_filter_size_2, s3_filter_size_3], stage=3, block='c')\n",
    "X = identity_block(X, 3, [s3_filter_size_1, s3_filter_size_2, s3_filter_size_3], stage=3, block='d')\n",
    "\n",
    "X = convolutional_block(X, f=3, filters=[s4_filter_size_1, s4_filter_size_2, s4_filter_size_3], stage=4, block='a', s=2)\n",
    "X = identity_block(X, 3, [s4_filter_size_1, s4_filter_size_2, s4_filter_size_3], stage=4, block='b')\n",
    "X = identity_block(X, 3, [s4_filter_size_1, s4_filter_size_2, s4_filter_size_3], stage=4, block='c')\n",
    "X = identity_block(X, 3, [s4_filter_size_1, s4_filter_size_2, s4_filter_size_3], stage=4, block='d')\n",
    "X = identity_block(X, 3, [s4_filter_size_1, s4_filter_size_2, s4_filter_size_3], stage=4, block='e')\n",
    "X = identity_block(X, 3, [s4_filter_size_1, s4_filter_size_2, s4_filter_size_3], stage=4, block='f')\n",
    "\n",
    "X = X = convolutional_block(X, f=3, filters=[s5_filter_size_1, s5_filter_size_2, s5_filter_size_3], stage=5, block='a', s=2)\n",
    "X = identity_block(X, 3, [s5_filter_size_1, s5_filter_size_2, s5_filter_size_3], stage=5, block='b')\n",
    "X = identity_block(X, 3, [s5_filter_size_1, s5_filter_size_2, s5_filter_size_3], stage=5, block='c')\n",
    "\n",
    "X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
    "\n",
    "X= Dropout(dp_size_1)(X)\n",
    "X = Flatten()(X)\n",
    "X=Dense(dense_size_1, activation='relu',kernel_regularizer=regularizers.l2(wd_size_1), name='fc1',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "X=Dense(dense_size_2, activation='relu',kernel_regularizer=regularizers.l2(wd_size_2), name='fc2',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "X= Dropout(dp_size_2)(X)\n",
    "X = Dense(10,activation='softmax', name='fc3',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "my_callbacks = [ tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "opt = Adam(learning_rate=learning_rate_1)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                optimizer=opt,\n",
    "                                metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_data=(x_valid,y_valid),\n",
    "          callbacks=[my_callbacks])                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
